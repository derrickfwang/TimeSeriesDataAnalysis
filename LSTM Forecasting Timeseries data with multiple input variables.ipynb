{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(r'C:\\...')\n",
    "os.getcwd()\n",
    "\n",
    "xl = pd.ExcelFile(\"....xlsx\")\n",
    "df1 = xl.parse(xl.sheet_names[0], skiprows=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['Amt'] = df1['Amt'].fillna(0)\n",
    "df1=df1.sort_values('Date').reset_index(drop=True)\n",
    "df1=df1.set_index(pd.DatetimeIndex(df1['Date'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "df1['NonBusDay'] = (df1['BankHoliday'] + df1['Weekend'])*10\n",
    "df1['BusDay'] = df1['WeekDay']\n",
    "df1['BusDay'][df1['WeekDay']>5] =0\n",
    "\n",
    "df2 = df1[['BusDay', 'NonBusDay', 'Amt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length: 182\n",
      "number of rows to truncate: 0\n",
      "Total number of weeks: 26\n"
     ]
    }
   ],
   "source": [
    "print('Data Length:', len(df2))\n",
    "print('number of rows to truncate:', len(df2)%7)\n",
    "print(\"Total number of weeks:\", len(df2)//7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 182, validation set from 126 to 154, test set from 154 to 182\n"
     ]
    }
   ],
   "source": [
    "lengthinWeeks=len(df2)//7\n",
    "valWeeks=4\n",
    "testWeeks=4\n",
    "test_index=(lengthinWeeks-testWeeks)*7 # taking the last 162 days to forecast on\n",
    "val_index=(lengthinWeeks-testWeeks-valWeeks)*7\n",
    "\n",
    "print('total {0}, validation set from {1} to {2}, test set from {3} to {4}'.format(lengthinWeeks*7, \\\n",
    "                                                                                   val_index,test_index, \\\n",
    "                                                                                   test_index, lengthinWeeks*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "y_scaled = preprocessing.scale(df2['Amt'].values)\n",
    "\n",
    "TrainMean = np.mean(df2['Amt'].values)\n",
    "TrainSd = np.std(df2['Amt'].values)\n",
    "\n",
    "def inverseScale(y_scaled, TrainMean, TrainSd):\n",
    "    return y_scaled*TrainSd + TrainMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df2['NormAmt'] = y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 736695.,  736726.,  736754.,  736785.,  736815.,  736846.,  736876.]),\n",
       " <a list of 7 Text xticklabel objects>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(data=df2, x=df2.index,y=\"NormAmt\")\n",
    "plt.xticks(rotation=90)\n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2[['BusDay', 'NonBusDay', 'NormAmt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to call that the casual observer model as cashflow of a given day is the same as the same weekday of previous week. \n",
    "# This model is lazy and assumes the cash flow is the same value it was 7 days ago.\n",
    "\n",
    "def casual_observer(series,step):\n",
    "    ypred=[series.iloc[i-step] for i in range(step,len(series))]\n",
    "    actual=[series.iloc[i+step] for i in range(len(series)-step)]\n",
    "    return (np.asarray(ypred).reshape(-1,1),np.asarray(actual).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple prediction Error: 1.27915025309\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler \n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "import statsmodels.api as sm \n",
    "from statsmodels.graphics.tsaplots import plot_pacf;\n",
    "\n",
    "step = 7\n",
    "temp_naive,actual=casual_observer(df3['NormAmt'].iloc[val_index:test_index],step)\n",
    "casual_error=mean_absolute_error(actual,temp_naive)\n",
    "print('simple prediction Error:',casual_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_prep(df,y_var,hist,frcst,frcst_step,val,test):\n",
    "    '''\n",
    "    inputs:\n",
    "    df: DataFrame that contains X, y variables\n",
    "    y_var: y_var label, this will be a string\n",
    "    hist: is an integer for the number of weeks lag\n",
    "    frcst: is an integer for the number of weeks forward for forecast\n",
    "    frcst_step: this is an integer for how many days to forecast   \n",
    "    Val: is the index for Validation\n",
    "    test: is the index for test\n",
    "    \n",
    "    outputs:\n",
    "    train_seqx: List of Arrays containing the training X input. The shape per array should be (7*hist,number of columns in df)\n",
    "    train_seqy: List of Arrays containing the training y (label), the shape per array should be (7*fore,)\n",
    "    val_seqx: List of Arrays containing the valdiation X input. The shape per array should be (7*hist,number of columns in df)\n",
    "    val_seqy: List of Arrays containing the validation y (label), the shape per array should be (7*fore,)\n",
    "    test_seqx: List of Arrays containing the test X input. The shape per array should be (7*hist,number of columns in df)\n",
    "    test_seqy: List of Arrays containing the test y (label), the shape per array should be (7*fore,)\n",
    "    '''\n",
    "    train_seqx=[]\n",
    "    train_seqy=[]\n",
    "    val_seqx=[]\n",
    "    val_seqy=[]\n",
    "    test_seqx=[]\n",
    "    test_seqy=[]\n",
    "    train_df=df.iloc[:val,:]\n",
    "    val_df=df.iloc[val:test,:]\n",
    "    test_df=df.iloc[test:,:]\n",
    "    train_x_arr=train_df.values\n",
    "    train_y_arr=train_df[y_var].values\n",
    "    val_x_arr=val_df.values\n",
    "    val_y_arr=val_df[y_var].values\n",
    "    test_x_arr=test_df.values\n",
    "    test_y_arr=test_df[y_var].values\n",
    "    hist_seq=7*hist # turning history into 24 hour x number of days vector\n",
    "    fore_seq=7*frcst\n",
    "    seq_len=(hist_seq+fore_seq)\n",
    "    \n",
    "    for i in range(len(train_y_arr)):\n",
    "        if i+seq_len<len(train_y_arr):\n",
    "            train_seqx.append(train_x_arr[i:i+hist_seq,:])\n",
    "            train_seqy.append(train_y_arr[i+hist_seq:i+seq_len][:frcst_step])         \n",
    "       \n",
    "        else:\n",
    "            val_index_add=i     \n",
    "            break\n",
    "            \n",
    "            \n",
    "    val_x_arr=np.insert(val_x_arr,0,train_x_arr[val_index_add:,:],axis=0)\n",
    "    val_y_arr=np.insert(val_y_arr,0,train_y_arr[val_index_add:],axis=0)\n",
    "    for i in range(len(val_y_arr)):\n",
    "        if i+seq_len<len(val_y_arr):\n",
    "            val_seqx.append(val_x_arr[i:i+hist_seq,:])\n",
    "            val_seqy.append(val_y_arr[i+hist_seq:i+seq_len][:frcst_step])       \n",
    "       \n",
    "        else:\n",
    "            test_index_add=i  \n",
    "            break\n",
    "             \n",
    "    test_x_arr=np.insert(test_x_arr,0,val_x_arr[test_index_add:,:],axis=0)\n",
    "    test_y_arr=np.insert(test_y_arr,0,val_y_arr[test_index_add:],axis=0)\n",
    "    for i in range(len(test_y_arr)):\n",
    "        if i+seq_len<len(test_y_arr):\n",
    "            test_seqx.append(test_x_arr[i:i+hist_seq,:])\n",
    "            test_seqy.append(test_y_arr[i+hist_seq:i+seq_len][:frcst_step])   \n",
    "    \n",
    "            \n",
    "    return train_seqx,train_seqy,val_seqx,val_seqy,test_seqx,test_seqy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_list, train_y_list,val_X_list, val_y_list,test_X_list, test_y_list=seq_prep(df3,\n",
    "                                                                                    \"NormAmt\",4,1,1,val_index,test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input sequence: (28, 3)\n",
      "Shape of output sequence: (1,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of input sequence:',train_X_list[0].shape)\n",
    "print('Shape of output sequence:',train_y_list[0].shape)\n",
    "\n",
    "# input x shape: 28 x 4\n",
    "#      4 weeks * 7 days = 28\n",
    "#      3 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Defining the sequence lengths for both input and output'''\n",
    "X_seqlen=train_X_list[0].shape[0]\n",
    "y_seqlen=train_y_list[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seqlen is  28\n",
      "y_seqlen is  1\n"
     ]
    }
   ],
   "source": [
    "print('X_seqlen is ', X_seqlen)\n",
    "print('y_seqlen is ', y_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pad_sequences function is a very versatile function. it allows us to handle cases \n",
    "of uneven sequence length by padding or truncating to a specificed length.\n",
    "Default pad value is 0, but user can apply any number or pass an impute into it\n",
    "\n",
    "'''\n",
    "train_X_seq = sequence.pad_sequences(train_X_list, dtype='float32', maxlen=X_seqlen, padding='post',truncating='post')\n",
    "train_y_seq = sequence.pad_sequences(train_y_list,dtype='float32',maxlen=y_seqlen, padding='post',truncating='post')\n",
    "val_X_seq = sequence.pad_sequences(val_X_list, dtype='float32', maxlen=X_seqlen, padding='post',truncating='post')\n",
    "val_y_seq = sequence.pad_sequences(val_y_list,dtype='float32',maxlen=y_seqlen, padding='post',truncating='post')\n",
    "test_X_seq = sequence.pad_sequences(test_X_list, dtype='float32', maxlen=X_seqlen, padding='post',truncating='post')\n",
    "test_y_seq = sequence.pad_sequences(test_y_list,dtype='float32',maxlen=y_seqlen, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (91, 28, 3)\n",
      "y shape: (91, 1)\n"
     ]
    }
   ],
   "source": [
    "'''WE got our Tensors'''\n",
    "print('X shape:', train_X_seq.shape)\n",
    "print('y shape:',train_y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: 91 (28, 3)\n",
      "y shape: 91 (1,)\n"
     ]
    }
   ],
   "source": [
    "'''before padding'''\n",
    "print('X shape:', len(train_X_list), train_X_list[0].shape)\n",
    "print('y shape:',len(train_y_list),train_y_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE =91\n",
    "NUM_TIMESTEPS=train_X_seq.shape[1]   #28 days\n",
    "features=train_X_seq.shape[2]        # number of variables\n",
    "output=train_y_seq.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,641\n",
      "Trainable params: 4,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "'''The input to an LSTM layer has to be 3D tensor. The default LSTM layer output is 2D '''\n",
    "model.add(LSTM(32, input_shape=(NUM_TIMESTEPS,features )))## The shape of each input sample is defined in 1st layer only\n",
    "model.add(Dense(output))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91 samples, validate on 28 samples\n",
      "Epoch 1/500\n",
      "91/91 [==============================] - 1s 8ms/step - loss: 0.5780 - val_loss: 0.9411\n",
      "Epoch 2/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.5416 - val_loss: 0.9291\n",
      "Epoch 3/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.5305 - val_loss: 0.9172\n",
      "Epoch 4/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.5209 - val_loss: 0.9138\n",
      "Epoch 5/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.5135 - val_loss: 0.9097\n",
      "Epoch 6/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.5073 - val_loss: 0.8998\n",
      "Epoch 7/500\n",
      "91/91 [==============================] - 0s 171us/step - loss: 0.5014 - val_loss: 0.8954\n",
      "Epoch 8/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4961 - val_loss: 0.8838\n",
      "Epoch 9/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4904 - val_loss: 0.8720\n",
      "Epoch 10/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4856 - val_loss: 0.8789\n",
      "Epoch 11/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4804 - val_loss: 0.8654\n",
      "Epoch 12/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4745 - val_loss: 0.8517\n",
      "Epoch 13/500\n",
      "91/91 [==============================] - 0s 160us/step - loss: 0.4688 - val_loss: 0.8520\n",
      "Epoch 14/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4633 - val_loss: 0.8367\n",
      "Epoch 15/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4575 - val_loss: 0.8456\n",
      "Epoch 16/500\n",
      "91/91 [==============================] - 0s 209us/step - loss: 0.4524 - val_loss: 0.8282\n",
      "Epoch 17/500\n",
      "91/91 [==============================] - 0s 198us/step - loss: 0.4458 - val_loss: 0.8202\n",
      "Epoch 18/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4399 - val_loss: 0.8129\n",
      "Epoch 19/500\n",
      "91/91 [==============================] - 0s 209us/step - loss: 0.4346 - val_loss: 0.7991\n",
      "Epoch 20/500\n",
      "91/91 [==============================] - 0s 177us/step - loss: 0.4358 - val_loss: 0.8620\n",
      "Epoch 21/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4603 - val_loss: 0.7935\n",
      "Epoch 22/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4414 - val_loss: 0.7993\n",
      "Epoch 23/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4260 - val_loss: 0.7902\n",
      "Epoch 24/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.4315 - val_loss: 0.8015\n",
      "Epoch 25/500\n",
      "91/91 [==============================] - 0s 182us/step - loss: 0.4264 - val_loss: 0.7857\n",
      "Epoch 26/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4357 - val_loss: 0.7889\n",
      "Epoch 27/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4201 - val_loss: 0.7853\n",
      "Epoch 28/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4205 - val_loss: 0.8107\n",
      "Epoch 29/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4286 - val_loss: 0.7789\n",
      "Epoch 30/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4296 - val_loss: 0.7854\n",
      "Epoch 31/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4166 - val_loss: 0.7804\n",
      "Epoch 32/500\n",
      "91/91 [==============================] - 0s 160us/step - loss: 0.4164 - val_loss: 0.8010\n",
      "Epoch 33/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4202 - val_loss: 0.7740\n",
      "Epoch 34/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.4245 - val_loss: 0.7797\n",
      "Epoch 35/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4127 - val_loss: 0.7759\n",
      "Epoch 36/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4135 - val_loss: 0.8094\n",
      "Epoch 37/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4223 - val_loss: 0.7726\n",
      "Epoch 38/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4259 - val_loss: 0.7760\n",
      "Epoch 39/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.4095 - val_loss: 0.7736\n",
      "Epoch 40/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4091 - val_loss: 0.8010\n",
      "Epoch 41/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4165 - val_loss: 0.7686\n",
      "Epoch 42/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4168 - val_loss: 0.7878\n",
      "Epoch 43/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4095 - val_loss: 0.7700\n",
      "Epoch 44/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4155 - val_loss: 0.7757\n",
      "Epoch 45/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4051 - val_loss: 0.7725\n",
      "Epoch 46/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4041 - val_loss: 0.7967\n",
      "Epoch 47/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4108 - val_loss: 0.7691\n",
      "Epoch 48/500\n",
      "91/91 [==============================] - 0s 134us/step - loss: 0.4175 - val_loss: 0.7752\n",
      "Epoch 49/500\n",
      "91/91 [==============================] - 0s 176us/step - loss: 0.4025 - val_loss: 0.7680\n",
      "Epoch 50/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.4042 - val_loss: 0.7962\n",
      "Epoch 51/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4085 - val_loss: 0.7678\n",
      "Epoch 52/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4141 - val_loss: 0.7754\n",
      "Epoch 53/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4000 - val_loss: 0.7652\n",
      "Epoch 54/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.4035 - val_loss: 0.7898\n",
      "Epoch 55/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.4034 - val_loss: 0.7670\n",
      "Epoch 56/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4082 - val_loss: 0.7845\n",
      "Epoch 57/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3994 - val_loss: 0.7658\n",
      "Epoch 58/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4094 - val_loss: 0.7760\n",
      "Epoch 59/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3958 - val_loss: 0.7667\n",
      "Epoch 60/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4023 - val_loss: 0.7850\n",
      "Epoch 61/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3979 - val_loss: 0.7641\n",
      "Epoch 62/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4065 - val_loss: 0.7761\n",
      "Epoch 63/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3938 - val_loss: 0.7641\n",
      "Epoch 64/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.4005 - val_loss: 0.7850\n",
      "Epoch 65/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3969 - val_loss: 0.7642\n",
      "Epoch 66/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4068 - val_loss: 0.7711\n",
      "Epoch 67/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3907 - val_loss: 0.7625\n",
      "Epoch 68/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3922 - val_loss: 0.7909\n",
      "Epoch 69/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3994 - val_loss: 0.7617\n",
      "Epoch 70/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.4061 - val_loss: 0.7664\n",
      "Epoch 71/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3878 - val_loss: 0.7601\n",
      "Epoch 72/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3911 - val_loss: 0.7929\n",
      "Epoch 73/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3996 - val_loss: 0.7594\n",
      "Epoch 74/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4015 - val_loss: 0.7679\n",
      "Epoch 75/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3859 - val_loss: 0.7585\n",
      "Epoch 76/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3877 - val_loss: 0.7835\n",
      "Epoch 77/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3944 - val_loss: 0.7574\n",
      "Epoch 78/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.4019 - val_loss: 0.7682\n",
      "Epoch 79/500\n",
      "91/91 [==============================] - 0s 137us/step - loss: 0.3843 - val_loss: 0.7588\n",
      "Epoch 80/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3874 - val_loss: 0.7843\n",
      "Epoch 81/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3936 - val_loss: 0.7574\n",
      "Epoch 82/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.4021 - val_loss: 0.7672\n",
      "Epoch 83/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3824 - val_loss: 0.7618\n",
      "Epoch 84/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3819 - val_loss: 0.7804\n",
      "Epoch 85/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3888 - val_loss: 0.7592\n",
      "Epoch 86/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3978 - val_loss: 0.7694\n",
      "Epoch 87/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3821 - val_loss: 0.7608\n",
      "Epoch 88/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3812 - val_loss: 0.7786\n",
      "Epoch 89/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3857 - val_loss: 0.7589\n",
      "Epoch 90/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3958 - val_loss: 0.7726\n",
      "Epoch 91/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3809 - val_loss: 0.7584\n",
      "Epoch 92/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3829 - val_loss: 0.7854\n",
      "Epoch 93/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3888 - val_loss: 0.7579\n",
      "Epoch 94/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3956 - val_loss: 0.7672\n",
      "Epoch 95/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3782 - val_loss: 0.7612\n",
      "Epoch 96/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3774 - val_loss: 0.7716\n",
      "Epoch 97/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3790 - val_loss: 0.7578\n",
      "Epoch 98/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3914 - val_loss: 0.7704\n",
      "Epoch 99/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3773 - val_loss: 0.7573\n",
      "Epoch 100/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3858 - val_loss: 0.7738\n",
      "Epoch 101/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3792 - val_loss: 0.7567\n",
      "Epoch 102/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3886 - val_loss: 0.7752\n",
      "Epoch 103/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3786 - val_loss: 0.7592\n",
      "Epoch 104/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3869 - val_loss: 0.7681\n",
      "Epoch 105/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3750 - val_loss: 0.7545\n",
      "Epoch 106/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3772 - val_loss: 0.7901\n",
      "Epoch 107/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3882 - val_loss: 0.7553\n",
      "Epoch 108/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3861 - val_loss: 0.7751\n",
      "Epoch 109/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3761 - val_loss: 0.7583\n",
      "Epoch 110/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3832 - val_loss: 0.7714\n",
      "Epoch 111/500\n",
      "91/91 [==============================] - 0s 134us/step - loss: 0.3740 - val_loss: 0.7550\n",
      "Epoch 112/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3774 - val_loss: 0.7868\n",
      "Epoch 113/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3823 - val_loss: 0.7576\n",
      "Epoch 114/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3843 - val_loss: 0.7761\n",
      "Epoch 115/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3746 - val_loss: 0.7608\n",
      "Epoch 116/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3690 - val_loss: 0.7653\n",
      "Epoch 117/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3704 - val_loss: 0.7562\n",
      "Epoch 118/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3810 - val_loss: 0.7759\n",
      "Epoch 119/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3739 - val_loss: 0.7569\n",
      "Epoch 120/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3804 - val_loss: 0.7781\n",
      "Epoch 121/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3738 - val_loss: 0.7588\n",
      "Epoch 122/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3785 - val_loss: 0.7774\n",
      "Epoch 123/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3727 - val_loss: 0.7570\n",
      "Epoch 124/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3724 - val_loss: 0.7854\n",
      "Epoch 125/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.3778 - val_loss: 0.7589\n",
      "Epoch 126/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3822 - val_loss: 0.7742\n",
      "Epoch 127/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3705 - val_loss: 0.7597\n",
      "Epoch 128/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3652 - val_loss: 0.7766\n",
      "Epoch 129/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3715 - val_loss: 0.7574\n",
      "Epoch 130/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3820 - val_loss: 0.7762\n",
      "Epoch 131/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3693 - val_loss: 0.7582\n",
      "Epoch 132/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3669 - val_loss: 0.7872\n",
      "Epoch 133/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3769 - val_loss: 0.7568\n",
      "Epoch 134/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3789 - val_loss: 0.7752\n",
      "Epoch 135/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3675 - val_loss: 0.7567\n",
      "Epoch 136/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3672 - val_loss: 0.7847\n",
      "Epoch 137/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3735 - val_loss: 0.7565\n",
      "Epoch 138/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3763 - val_loss: 0.7770\n",
      "Epoch 139/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3672 - val_loss: 0.7576\n",
      "Epoch 140/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3678 - val_loss: 0.7817\n",
      "Epoch 141/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3692 - val_loss: 0.7571\n",
      "Epoch 142/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3706 - val_loss: 0.7845\n",
      "Epoch 143/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3696 - val_loss: 0.7610\n",
      "Epoch 144/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3702 - val_loss: 0.7779\n",
      "Epoch 145/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3660 - val_loss: 0.7585\n",
      "Epoch 146/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3606 - val_loss: 0.7780\n",
      "Epoch 147/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3682 - val_loss: 0.7570\n",
      "Epoch 148/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3760 - val_loss: 0.7734\n",
      "Epoch 149/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3627 - val_loss: 0.7562\n",
      "Epoch 150/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3607 - val_loss: 0.7916\n",
      "Epoch 151/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3741 - val_loss: 0.7550\n",
      "Epoch 152/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3720 - val_loss: 0.7767\n",
      "Epoch 153/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3629 - val_loss: 0.7541\n",
      "Epoch 154/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3640 - val_loss: 0.7863\n",
      "Epoch 155/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3692 - val_loss: 0.7561\n",
      "Epoch 156/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3707 - val_loss: 0.7747\n",
      "Epoch 157/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3618 - val_loss: 0.7533\n",
      "Epoch 158/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3606 - val_loss: 0.7865\n",
      "Epoch 159/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3700 - val_loss: 0.7538\n",
      "Epoch 160/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3701 - val_loss: 0.7738\n",
      "Epoch 161/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3608 - val_loss: 0.7516\n",
      "Epoch 162/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3622 - val_loss: 0.7803\n",
      "Epoch 163/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3656 - val_loss: 0.7535\n",
      "Epoch 164/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3688 - val_loss: 0.7718\n",
      "Epoch 165/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3598 - val_loss: 0.7520\n",
      "Epoch 166/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3601 - val_loss: 0.7766\n",
      "Epoch 167/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3637 - val_loss: 0.7525\n",
      "Epoch 168/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3676 - val_loss: 0.7706\n",
      "Epoch 169/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3592 - val_loss: 0.7519\n",
      "Epoch 170/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3624 - val_loss: 0.7719\n",
      "Epoch 171/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3600 - val_loss: 0.7515\n",
      "Epoch 172/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3617 - val_loss: 0.7760\n",
      "Epoch 173/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3620 - val_loss: 0.7513\n",
      "Epoch 174/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3630 - val_loss: 0.7735\n",
      "Epoch 175/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3606 - val_loss: 0.7524\n",
      "Epoch 176/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3631 - val_loss: 0.7692\n",
      "Epoch 177/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3588 - val_loss: 0.7513\n",
      "Epoch 178/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3609 - val_loss: 0.7734\n",
      "Epoch 179/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3605 - val_loss: 0.7513\n",
      "Epoch 180/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3607 - val_loss: 0.7727\n",
      "Epoch 181/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3599 - val_loss: 0.7516\n",
      "Epoch 182/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3607 - val_loss: 0.7706\n",
      "Epoch 183/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3594 - val_loss: 0.7515\n",
      "Epoch 184/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3602 - val_loss: 0.7683\n",
      "Epoch 185/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3577 - val_loss: 0.7506\n",
      "Epoch 186/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3520 - val_loss: 0.7665\n",
      "Epoch 187/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3601 - val_loss: 0.7513\n",
      "Epoch 188/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3652 - val_loss: 0.7677\n",
      "Epoch 189/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3567 - val_loss: 0.7511\n",
      "Epoch 190/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3591 - val_loss: 0.7710\n",
      "Epoch 191/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3585 - val_loss: 0.7495\n",
      "Epoch 192/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3592 - val_loss: 0.7725\n",
      "Epoch 193/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3590 - val_loss: 0.7499\n",
      "Epoch 194/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3579 - val_loss: 0.7690\n",
      "Epoch 195/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3580 - val_loss: 0.7483\n",
      "Epoch 196/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3583 - val_loss: 0.7727\n",
      "Epoch 197/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3595 - val_loss: 0.7504\n",
      "Epoch 198/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3581 - val_loss: 0.7639\n",
      "Epoch 199/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3559 - val_loss: 0.7460\n",
      "Epoch 200/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3515 - val_loss: 0.7618\n",
      "Epoch 201/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3574 - val_loss: 0.7514\n",
      "Epoch 202/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.3634 - val_loss: 0.7616\n",
      "Epoch 203/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3543 - val_loss: 0.7502\n",
      "Epoch 204/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3576 - val_loss: 0.7644\n",
      "Epoch 205/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3559 - val_loss: 0.7462\n",
      "Epoch 206/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3552 - val_loss: 0.7565\n",
      "Epoch 207/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3522 - val_loss: 0.7499\n",
      "Epoch 208/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3596 - val_loss: 0.7643\n",
      "Epoch 209/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3553 - val_loss: 0.7474\n",
      "Epoch 210/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3570 - val_loss: 0.7697\n",
      "Epoch 211/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3569 - val_loss: 0.7463\n",
      "Epoch 212/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3553 - val_loss: 0.7680\n",
      "Epoch 213/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3578 - val_loss: 0.7458\n",
      "Epoch 214/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3551 - val_loss: 0.7556\n",
      "Epoch 215/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3514 - val_loss: 0.7482\n",
      "Epoch 216/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3574 - val_loss: 0.7637\n",
      "Epoch 217/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3545 - val_loss: 0.7475\n",
      "Epoch 218/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3570 - val_loss: 0.7644\n",
      "Epoch 219/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3544 - val_loss: 0.7460\n",
      "Epoch 220/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3449 - val_loss: 0.7502\n",
      "Epoch 221/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3486 - val_loss: 0.7520\n",
      "Epoch 222/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3633 - val_loss: 0.7556\n",
      "Epoch 223/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3513 - val_loss: 0.7515\n",
      "Epoch 224/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3591 - val_loss: 0.7543\n",
      "Epoch 225/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3503 - val_loss: 0.7460\n",
      "Epoch 226/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3542 - val_loss: 0.7629\n",
      "Epoch 227/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3547 - val_loss: 0.7448\n",
      "Epoch 228/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3565 - val_loss: 0.7626\n",
      "Epoch 229/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3539 - val_loss: 0.7448\n",
      "Epoch 230/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3531 - val_loss: 0.7542\n",
      "Epoch 231/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3510 - val_loss: 0.7404\n",
      "Epoch 232/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.3472 - val_loss: 0.7523\n",
      "Epoch 233/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3486 - val_loss: 0.7415\n",
      "Epoch 234/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3510 - val_loss: 0.7465\n",
      "Epoch 235/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3438 - val_loss: 0.7502\n",
      "Epoch 236/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3620 - val_loss: 0.7607\n",
      "Epoch 237/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3532 - val_loss: 0.7510\n",
      "Epoch 238/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3599 - val_loss: 0.7511\n",
      "Epoch 239/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3476 - val_loss: 0.7444\n",
      "Epoch 240/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3490 - val_loss: 0.7669\n",
      "Epoch 241/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3569 - val_loss: 0.7426\n",
      "Epoch 242/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3543 - val_loss: 0.7651\n",
      "Epoch 243/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.3532 - val_loss: 0.7441\n",
      "Epoch 244/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3516 - val_loss: 0.7552\n",
      "Epoch 245/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3498 - val_loss: 0.7408\n",
      "Epoch 246/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3491 - val_loss: 0.7554\n",
      "Epoch 247/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3506 - val_loss: 0.7391\n",
      "Epoch 248/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3488 - val_loss: 0.7484\n",
      "Epoch 249/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3429 - val_loss: 0.7408\n",
      "Epoch 250/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3477 - val_loss: 0.7495\n",
      "Epoch 251/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3482 - val_loss: 0.7417\n",
      "Epoch 252/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3517 - val_loss: 0.7628\n",
      "Epoch 253/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3543 - val_loss: 0.7431\n",
      "Epoch 254/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3525 - val_loss: 0.7555\n",
      "Epoch 255/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3493 - val_loss: 0.7408\n",
      "Epoch 256/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3467 - val_loss: 0.7599\n",
      "Epoch 257/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3536 - val_loss: 0.7415\n",
      "Epoch 258/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3533 - val_loss: 0.7528\n",
      "Epoch 259/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3480 - val_loss: 0.7399\n",
      "Epoch 260/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3470 - val_loss: 0.7435\n",
      "Epoch 261/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3416 - val_loss: 0.7458\n",
      "Epoch 262/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3554 - val_loss: 0.7468\n",
      "Epoch 263/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3450 - val_loss: 0.7419\n",
      "Epoch 264/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3504 - val_loss: 0.7525\n",
      "Epoch 265/500\n",
      "91/91 [==============================] - 0s 165us/step - loss: 0.3487 - val_loss: 0.7394\n",
      "Epoch 266/500\n",
      "91/91 [==============================] - 0s 187us/step - loss: 0.3516 - val_loss: 0.7548\n",
      "Epoch 267/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3481 - val_loss: 0.7400\n",
      "Epoch 268/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3450 - val_loss: 0.7590\n",
      "Epoch 269/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3508 - val_loss: 0.7404\n",
      "Epoch 270/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3497 - val_loss: 0.7512\n",
      "Epoch 271/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.3464 - val_loss: 0.7377\n",
      "Epoch 272/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3428 - val_loss: 0.7606\n",
      "Epoch 273/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3517 - val_loss: 0.7379\n",
      "Epoch 274/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3506 - val_loss: 0.7540\n",
      "Epoch 275/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3467 - val_loss: 0.7403\n",
      "Epoch 276/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3406 - val_loss: 0.7643\n",
      "Epoch 277/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3550 - val_loss: 0.7362\n",
      "Epoch 278/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3471 - val_loss: 0.7514\n",
      "Epoch 279/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3445 - val_loss: 0.7389\n",
      "Epoch 280/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3447 - val_loss: 0.7562\n",
      "Epoch 281/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3480 - val_loss: 0.7372\n",
      "Epoch 282/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3468 - val_loss: 0.7535\n",
      "Epoch 283/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3465 - val_loss: 0.7366\n",
      "Epoch 284/500\n",
      "91/91 [==============================] - 0s 133us/step - loss: 0.3456 - val_loss: 0.7535\n",
      "Epoch 285/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3466 - val_loss: 0.7367\n",
      "Epoch 286/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3407 - val_loss: 0.7607\n",
      "Epoch 287/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3528 - val_loss: 0.7358\n",
      "Epoch 288/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3479 - val_loss: 0.7515\n",
      "Epoch 289/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3448 - val_loss: 0.7371\n",
      "Epoch 290/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3377 - val_loss: 0.7538\n",
      "Epoch 291/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3439 - val_loss: 0.7338\n",
      "Epoch 292/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3455 - val_loss: 0.7431\n",
      "Epoch 293/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3363 - val_loss: 0.7418\n",
      "Epoch 294/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3445 - val_loss: 0.7608\n",
      "Epoch 295/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3518 - val_loss: 0.7381\n",
      "Epoch 296/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3509 - val_loss: 0.7488\n",
      "Epoch 297/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3422 - val_loss: 0.7387\n",
      "Epoch 298/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3484 - val_loss: 0.7497\n",
      "Epoch 299/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3427 - val_loss: 0.7379\n",
      "Epoch 300/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3471 - val_loss: 0.7489\n",
      "Epoch 301/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3425 - val_loss: 0.7362\n",
      "Epoch 302/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3424 - val_loss: 0.7567\n",
      "Epoch 303/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3472 - val_loss: 0.7355\n",
      "Epoch 304/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3461 - val_loss: 0.7506\n",
      "Epoch 305/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3431 - val_loss: 0.7350\n",
      "Epoch 306/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3426 - val_loss: 0.7519\n",
      "Epoch 307/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3447 - val_loss: 0.7331\n",
      "Epoch 308/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3452 - val_loss: 0.7544\n",
      "Epoch 309/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3454 - val_loss: 0.7349\n",
      "Epoch 310/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3418 - val_loss: 0.7516\n",
      "Epoch 311/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3445 - val_loss: 0.7329\n",
      "Epoch 312/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3437 - val_loss: 0.7494\n",
      "Epoch 313/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3433 - val_loss: 0.7325\n",
      "Epoch 314/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3405 - val_loss: 0.7490\n",
      "Epoch 315/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3424 - val_loss: 0.7336\n",
      "Epoch 316/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3438 - val_loss: 0.7503\n",
      "Epoch 317/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3429 - val_loss: 0.7332\n",
      "Epoch 318/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3405 - val_loss: 0.7501\n",
      "Epoch 319/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3432 - val_loss: 0.7328\n",
      "Epoch 320/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3399 - val_loss: 0.7530\n",
      "Epoch 321/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3460 - val_loss: 0.7324\n",
      "Epoch 322/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3465 - val_loss: 0.7498\n",
      "Epoch 323/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3416 - val_loss: 0.7370\n",
      "Epoch 324/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3347 - val_loss: 0.7587\n",
      "Epoch 325/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3499 - val_loss: 0.7305\n",
      "Epoch 326/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3435 - val_loss: 0.7458\n",
      "Epoch 327/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3388 - val_loss: 0.7360\n",
      "Epoch 328/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3460 - val_loss: 0.7437\n",
      "Epoch 329/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3390 - val_loss: 0.7344\n",
      "Epoch 330/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3442 - val_loss: 0.7433\n",
      "Epoch 331/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3397 - val_loss: 0.7318\n",
      "Epoch 332/500\n",
      "91/91 [==============================] - 0s 131us/step - loss: 0.3429 - val_loss: 0.7447\n",
      "Epoch 333/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3409 - val_loss: 0.7304\n",
      "Epoch 334/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3411 - val_loss: 0.7464\n",
      "Epoch 335/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3421 - val_loss: 0.7310\n",
      "Epoch 336/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3411 - val_loss: 0.7447\n",
      "Epoch 337/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3416 - val_loss: 0.7298\n",
      "Epoch 338/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3397 - val_loss: 0.7411\n",
      "Epoch 339/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3379 - val_loss: 0.7335\n",
      "Epoch 340/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3482 - val_loss: 0.7394\n",
      "Epoch 341/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3369 - val_loss: 0.7351\n",
      "Epoch 342/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3376 - val_loss: 0.7461\n",
      "Epoch 343/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3424 - val_loss: 0.7297\n",
      "Epoch 344/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3438 - val_loss: 0.7447\n",
      "Epoch 345/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3394 - val_loss: 0.7294\n",
      "Epoch 346/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3389 - val_loss: 0.7454\n",
      "Epoch 347/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3417 - val_loss: 0.7290\n",
      "Epoch 348/500\n",
      "91/91 [==============================] - 0s 129us/step - loss: 0.3417 - val_loss: 0.7403\n",
      "Epoch 349/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3391 - val_loss: 0.7310\n",
      "Epoch 350/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3326 - val_loss: 0.7471\n",
      "Epoch 351/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3433 - val_loss: 0.7286\n",
      "Epoch 352/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3431 - val_loss: 0.7391\n",
      "Epoch 353/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3347 - val_loss: 0.7344\n",
      "Epoch 354/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3390 - val_loss: 0.7414\n",
      "Epoch 355/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3381 - val_loss: 0.7313\n",
      "Epoch 356/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.3444 - val_loss: 0.7387\n",
      "Epoch 357/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3362 - val_loss: 0.7319\n",
      "Epoch 358/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3342 - val_loss: 0.7481\n",
      "Epoch 359/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3434 - val_loss: 0.7261\n",
      "Epoch 360/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3427 - val_loss: 0.7412\n",
      "Epoch 361/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3371 - val_loss: 0.7302\n",
      "Epoch 362/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3388 - val_loss: 0.7439\n",
      "Epoch 363/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3400 - val_loss: 0.7273\n",
      "Epoch 364/500\n",
      "91/91 [==============================] - 0s 128us/step - loss: 0.3430 - val_loss: 0.7342\n",
      "Epoch 365/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3316 - val_loss: 0.7336\n",
      "Epoch 366/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3357 - val_loss: 0.7413\n",
      "Epoch 367/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3381 - val_loss: 0.7296\n",
      "Epoch 368/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3445 - val_loss: 0.7357\n",
      "Epoch 369/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3350 - val_loss: 0.7314\n",
      "Epoch 370/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3307 - val_loss: 0.7371\n",
      "Epoch 371/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3359 - val_loss: 0.7273\n",
      "Epoch 372/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3377 - val_loss: 0.7466\n",
      "Epoch 373/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3426 - val_loss: 0.7277\n",
      "Epoch 374/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3431 - val_loss: 0.7389\n",
      "Epoch 375/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3360 - val_loss: 0.7291\n",
      "Epoch 376/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3359 - val_loss: 0.7422\n",
      "Epoch 377/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3401 - val_loss: 0.7268\n",
      "Epoch 378/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3390 - val_loss: 0.7375\n",
      "Epoch 379/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3368 - val_loss: 0.7266\n",
      "Epoch 380/500\n",
      "91/91 [==============================] - 0s 130us/step - loss: 0.3376 - val_loss: 0.7396\n",
      "Epoch 381/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3385 - val_loss: 0.7260\n",
      "Epoch 382/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3402 - val_loss: 0.7296\n",
      "Epoch 383/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3282 - val_loss: 0.7301\n",
      "Epoch 384/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3372 - val_loss: 0.7361\n",
      "Epoch 385/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3358 - val_loss: 0.7286\n",
      "Epoch 386/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3425 - val_loss: 0.7335\n",
      "Epoch 387/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3330 - val_loss: 0.7279\n",
      "Epoch 388/500\n",
      "91/91 [==============================] - 0s 116us/step - loss: 0.3387 - val_loss: 0.7382\n",
      "Epoch 389/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3365 - val_loss: 0.7268\n",
      "Epoch 390/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3408 - val_loss: 0.7336\n",
      "Epoch 391/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3344 - val_loss: 0.7300\n",
      "Epoch 392/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3291 - val_loss: 0.7343\n",
      "Epoch 393/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3355 - val_loss: 0.7264\n",
      "Epoch 394/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3412 - val_loss: 0.7351\n",
      "Epoch 395/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3341 - val_loss: 0.7264\n",
      "Epoch 396/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3301 - val_loss: 0.7428\n",
      "Epoch 397/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3416 - val_loss: 0.7218\n",
      "Epoch 398/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3370 - val_loss: 0.7338\n",
      "Epoch 399/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3319 - val_loss: 0.7295\n",
      "Epoch 400/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3448 - val_loss: 0.7301\n",
      "Epoch 401/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3293 - val_loss: 0.7285\n",
      "Epoch 402/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3323 - val_loss: 0.7359\n",
      "Epoch 403/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3356 - val_loss: 0.7255\n",
      "Epoch 404/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3403 - val_loss: 0.7354\n",
      "Epoch 405/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3330 - val_loss: 0.7262\n",
      "Epoch 406/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3417 - val_loss: 0.7291\n",
      "Epoch 407/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3316 - val_loss: 0.7274\n",
      "Epoch 408/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3265 - val_loss: 0.7250\n",
      "Epoch 409/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3347 - val_loss: 0.7342\n",
      "Epoch 410/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3315 - val_loss: 0.7257\n",
      "Epoch 411/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3412 - val_loss: 0.7350\n",
      "Epoch 412/500\n",
      "91/91 [==============================] - 0s 140us/step - loss: 0.3332 - val_loss: 0.7265\n",
      "Epoch 413/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3421 - val_loss: 0.7299\n",
      "Epoch 414/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3299 - val_loss: 0.7245\n",
      "Epoch 415/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3285 - val_loss: 0.7441\n",
      "Epoch 416/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3406 - val_loss: 0.7211\n",
      "Epoch 417/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3372 - val_loss: 0.7393\n",
      "Epoch 418/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3359 - val_loss: 0.7222\n",
      "Epoch 419/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3379 - val_loss: 0.7355\n",
      "Epoch 420/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3326 - val_loss: 0.7235\n",
      "Epoch 421/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3363 - val_loss: 0.7306\n",
      "Epoch 422/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3319 - val_loss: 0.7226\n",
      "Epoch 423/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3378 - val_loss: 0.7279\n",
      "Epoch 424/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3309 - val_loss: 0.7260\n",
      "Epoch 425/500\n",
      "91/91 [==============================] - 0s 154us/step - loss: 0.3258 - val_loss: 0.7196\n",
      "Epoch 426/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3333 - val_loss: 0.7347\n",
      "Epoch 427/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3316 - val_loss: 0.7190\n",
      "Epoch 428/500\n",
      "91/91 [==============================] - 0s 149us/step - loss: 0.3378 - val_loss: 0.7353\n",
      "Epoch 429/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3348 - val_loss: 0.7229\n",
      "Epoch 430/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3389 - val_loss: 0.7273\n",
      "Epoch 431/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3297 - val_loss: 0.7187\n",
      "Epoch 432/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3244 - val_loss: 0.7377\n",
      "Epoch 433/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3373 - val_loss: 0.7203\n",
      "Epoch 434/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3368 - val_loss: 0.7351\n",
      "Epoch 435/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3342 - val_loss: 0.7214\n",
      "Epoch 436/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3380 - val_loss: 0.7301\n",
      "Epoch 437/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3297 - val_loss: 0.7215\n",
      "Epoch 438/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3370 - val_loss: 0.7285\n",
      "Epoch 439/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3292 - val_loss: 0.7199\n",
      "Epoch 440/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3308 - val_loss: 0.7332\n",
      "Epoch 441/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3336 - val_loss: 0.7219\n",
      "Epoch 442/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3362 - val_loss: 0.7286\n",
      "Epoch 443/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3294 - val_loss: 0.7198\n",
      "Epoch 444/500\n",
      "91/91 [==============================] - 0s 137us/step - loss: 0.3390 - val_loss: 0.7272\n",
      "Epoch 445/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3287 - val_loss: 0.7218\n",
      "Epoch 446/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3236 - val_loss: 0.7216\n",
      "Epoch 447/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3311 - val_loss: 0.7320\n",
      "Epoch 448/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3312 - val_loss: 0.7145\n",
      "Epoch 449/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3354 - val_loss: 0.7245\n",
      "Epoch 450/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3246 - val_loss: 0.7177\n",
      "Epoch 451/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3350 - val_loss: 0.7215\n",
      "Epoch 452/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3238 - val_loss: 0.7323\n",
      "Epoch 453/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3343 - val_loss: 0.7209\n",
      "Epoch 454/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3398 - val_loss: 0.7294\n",
      "Epoch 455/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3301 - val_loss: 0.7219\n",
      "Epoch 456/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3403 - val_loss: 0.7253\n",
      "Epoch 457/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3273 - val_loss: 0.7171\n",
      "Epoch 458/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3225 - val_loss: 0.7411\n",
      "Epoch 459/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3408 - val_loss: 0.7155\n",
      "Epoch 460/500\n",
      "91/91 [==============================] - 0s 134us/step - loss: 0.3333 - val_loss: 0.7316\n",
      "Epoch 461/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3324 - val_loss: 0.7189\n",
      "Epoch 462/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3363 - val_loss: 0.7256\n",
      "Epoch 463/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3281 - val_loss: 0.7181\n",
      "Epoch 464/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3339 - val_loss: 0.7283\n",
      "Epoch 465/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3300 - val_loss: 0.7179\n",
      "Epoch 466/500\n",
      "91/91 [==============================] - 0s 138us/step - loss: 0.3382 - val_loss: 0.7244\n",
      "Epoch 467/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3267 - val_loss: 0.7186\n",
      "Epoch 468/500\n",
      "91/91 [==============================] - 0s 133us/step - loss: 0.3221 - val_loss: 0.7193\n",
      "Epoch 469/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3292 - val_loss: 0.7275\n",
      "Epoch 470/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3288 - val_loss: 0.7117\n",
      "Epoch 471/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3348 - val_loss: 0.7294\n",
      "Epoch 472/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3319 - val_loss: 0.7192\n",
      "Epoch 473/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3367 - val_loss: 0.7238\n",
      "Epoch 474/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3267 - val_loss: 0.7183\n",
      "Epoch 475/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3343 - val_loss: 0.7264\n",
      "Epoch 476/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3286 - val_loss: 0.7179\n",
      "Epoch 477/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3385 - val_loss: 0.7240\n",
      "Epoch 478/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3272 - val_loss: 0.7240\n",
      "Epoch 479/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3276 - val_loss: 0.7163\n",
      "Epoch 480/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3336 - val_loss: 0.7208\n",
      "Epoch 481/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3211 - val_loss: 0.7122\n",
      "Epoch 482/500\n",
      "91/91 [==============================] - 0s 149us/step - loss: 0.3313 - val_loss: 0.7268\n",
      "Epoch 483/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3254 - val_loss: 0.7190\n",
      "Epoch 484/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3420 - val_loss: 0.7252\n",
      "Epoch 485/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3250 - val_loss: 0.7153\n",
      "Epoch 486/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3322 - val_loss: 0.7312\n",
      "Epoch 487/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3301 - val_loss: 0.7195\n",
      "Epoch 488/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3374 - val_loss: 0.7210\n",
      "Epoch 489/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3244 - val_loss: 0.7185\n",
      "Epoch 490/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3349 - val_loss: 0.7219\n",
      "Epoch 491/500\n",
      "91/91 [==============================] - 0s 125us/step - loss: 0.3253 - val_loss: 0.7182\n",
      "Epoch 492/500\n",
      "91/91 [==============================] - 0s 121us/step - loss: 0.3352 - val_loss: 0.7211\n",
      "Epoch 493/500\n",
      "91/91 [==============================] - 0s 143us/step - loss: 0.3245 - val_loss: 0.7179\n",
      "Epoch 494/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3230 - val_loss: 0.7202\n",
      "Epoch 495/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3276 - val_loss: 0.7259\n",
      "Epoch 496/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3283 - val_loss: 0.7104\n",
      "Epoch 497/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3350 - val_loss: 0.7205\n",
      "Epoch 498/500\n",
      "91/91 [==============================] - 0s 127us/step - loss: 0.3213 - val_loss: 0.7148\n",
      "Epoch 499/500\n",
      "91/91 [==============================] - 0s 131us/step - loss: 0.3333 - val_loss: 0.7226\n",
      "Epoch 500/500\n",
      "91/91 [==============================] - 0s 132us/step - loss: 0.3229 - val_loss: 0.7154\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_X_seq,train_y_seq,epochs=500,shuffle=False,batch_size=BATCH_SIZE,validation_data=(val_X_seq,val_y_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFyCAYAAAB2hOkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYoGRVEURVGCQkWDoiiKoihB8f9Hez4kTY1rYAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ab47879a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "#plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71539557"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time mean_absolute_error(val_y_seq,model.predict(val_X_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.51 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.63634777"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time mean_absolute_error(test_y_seq,model.predict(test_X_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ab478dddd8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_actual =inverseScale(train_y_seq, TrainMean, TrainSd)\n",
    "train_Pred =inverseScale(model.predict(train_X_seq), TrainMean, TrainSd)\n",
    "days = range(len(train_actual))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(days, train_actual, 'b', label='Actual')\n",
    "plt.plot(days, train_Pred, 'r', label='Pred')\n",
    "plt.title('Training Set Prediction')\n",
    "plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ab478b12e8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_actual =inverseScale(val_y_seq, TrainMean, TrainSd)\n",
    "val_Pred =inverseScale(model.predict(val_X_seq), TrainMean, TrainSd)\n",
    "days = range(len(val_actual))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(days, val_actual, 'b', label='Actual')\n",
    "plt.plot(days, val_Pred, 'r', label='Pred')\n",
    "plt.title('Validation Set Prediction')\n",
    "plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ab479cde80>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_actual =inverseScale(test_y_seq, TrainMean, TrainSd)\n",
    "test_Pred =inverseScale(model.predict(test_X_seq), TrainMean, TrainSd)\n",
    "days = range(len(test_actual))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(days, test_actual, 'b', label='Actual')\n",
    "plt.plot(days, test_Pred, 'r', label='Pred')\n",
    "plt.title('Test Set Prediction')\n",
    "plt.legend(loc=2)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
